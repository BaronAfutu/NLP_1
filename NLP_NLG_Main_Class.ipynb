{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP NLG Main Class.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GML - NLP\n",
        "## NLG"
      ],
      "metadata": {
        "id": "A0aYgdXNV1lG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np "
      ],
      "metadata": {
        "id": "yOyQjT3LWkMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing Data: Tiny Shakespeare\n",
        "**Description:**\n",
        "40,000 lines of Shakespeare from a variety of Shakespeare's plays.\n",
        "From: [Tensorflow Datasets - Tiny Shakespeare](https://www.tensorflow.org/datasets/catalog/tiny_shakespeare)"
      ],
      "metadata": {
        "id": "FdO6mfQLGBlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preparing data\n",
        "!wget --no-check-certificate \\\n",
        "  https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt \\\n",
        "  -O /tmp/tiny_shakespeare.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVadC82vFEPG",
        "outputId": "d7deb44c-aabf-4e62-a091-c3b737abd0ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-07-01 10:49:49--  https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.214.128, 173.194.215.128, 173.194.217.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.214.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘/tmp/tiny_shakespeare.txt’\n",
            "\n",
            "\r          /tmp/tiny   0%[                    ]       0  --.-KB/s               \r/tmp/tiny_shakespea 100%[===================>]   1.06M  --.-KB/s    in 0.008s  \n",
            "\n",
            "2022-07-01 10:49:49 (127 MB/s) - ‘/tmp/tiny_shakespeare.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "data = open('/tmp/tiny_shakespeare.txt').read()\n",
        "dataList = data.lower().split('\\n')\n",
        "# split the paragraphs (or block) of text into lines (sentences)\n",
        "\n",
        "dataList[:25]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJRJxH2JWoq3",
        "outputId": "16595ed1-eee3-42be-b071-5c12583da30e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['first citizen:',\n",
              " 'before we proceed any further, hear me speak.',\n",
              " '',\n",
              " 'all:',\n",
              " 'speak, speak.',\n",
              " '',\n",
              " 'first citizen:',\n",
              " 'you are all resolved rather to die than to famish?',\n",
              " '',\n",
              " 'all:',\n",
              " 'resolved. resolved.',\n",
              " '',\n",
              " 'first citizen:',\n",
              " 'first, you know caius marcius is chief enemy to the people.',\n",
              " '',\n",
              " 'all:',\n",
              " \"we know't, we know't.\",\n",
              " '',\n",
              " 'first citizen:',\n",
              " \"let us kill him, and we'll have corn at our own price.\",\n",
              " \"is't a verdict?\",\n",
              " '',\n",
              " 'all:',\n",
              " \"no more talking on't; let it be done: away, away!\",\n",
              " '']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = list(filter(lambda data: ':' not in data and data!='', dataList))\n",
        "# filtering empty lines and speaker names out\n",
        "\n",
        "print(\"Total number of lines: \",len(corpus))\n",
        "\n",
        "corpus = corpus[:1500]\n",
        "corpus[:25]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fx88-mw0ONPE",
        "outputId": "25883615-bb69-4c03-93d1-9361f44c547d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of lines:  22500\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['before we proceed any further, hear me speak.',\n",
              " 'speak, speak.',\n",
              " 'you are all resolved rather to die than to famish?',\n",
              " 'resolved. resolved.',\n",
              " 'first, you know caius marcius is chief enemy to the people.',\n",
              " \"we know't, we know't.\",\n",
              " \"let us kill him, and we'll have corn at our own price.\",\n",
              " \"is't a verdict?\",\n",
              " 'one word, good citizens.',\n",
              " 'we are accounted poor citizens, the patricians good.',\n",
              " 'would yield us but the superfluity, while it were',\n",
              " 'wholesome, we might guess they relieved us humanely;',\n",
              " 'afflicts us, the object of our misery, is as an',\n",
              " 'inventory to particularise their abundance; our',\n",
              " 'sufferance is a gain to them let us revenge this with',\n",
              " 'speak this in hunger for bread, not in thirst for revenge.',\n",
              " 'would you proceed especially against caius marcius?',\n",
              " 'consider you what services he has done for his country?',\n",
              " 'very well; and could be content to give him good',\n",
              " 'report fort, but that he pays himself with being proud.',\n",
              " 'nay, but speak not maliciously.',\n",
              " 'i say unto you, what he hath done famously, he did',\n",
              " 'content to say it was for his country he did it to',\n",
              " 'please his mother and to be partly proud; which he',\n",
              " 'is, even till the altitude of his virtue.']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing The Model Input\n",
        "For each line in the corpus, we want to generate training data in the form a stream; one word at a time.\n",
        "Help the model learn which word possibly could appear after a given word or set of words.\n",
        "\n",
        "Eg. Given the token sequence\n",
        ">[5, 6, 7, 2, 8, 9, 10]\n",
        "\n",
        "The resulting training data becomes\n",
        ">[5,6]\\\n",
        "[5,6,7]\\\n",
        "[5, 6, 7, 2]\\\n",
        "[5, 6, 7, 2, 8]\\\n",
        "[5, 6, 7, 2, 8, 9]\\\n",
        "[5, 6, 7, 2, 8, 9, 10]\n",
        "\n",
        "The first (n-1) tokens are used as input, then the nth token as output to train the system.\n",
        "\n",
        "The tells the system, if the input was \"in\"(5), the next word (output) is \"the\"(6). And if the input was \"in the\" the next word is \"town\". This goes on till the whole sentence is learned"
      ],
      "metadata": {
        "id": "U65J6f7Yw_1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index)+1 \n",
        "\n",
        "total_words"
      ],
      "metadata": {
        "id": "lfKgBG9HZfyo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a16fb176-e4af-4d0d-de0b-cef608a22942"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2316"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputSequences = []\n",
        "\n",
        "for line in corpus: \n",
        "  token_list = tokenizer.texts_to_sequences([line])[0] \n",
        "  for i in range(1,len(token_list)):\n",
        "    nGramSequences = token_list[:i+1]\n",
        "    inputSequences.append(nGramSequences)\n",
        "\n",
        "inputSequences[:25]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S49YyFBlYjfc",
        "outputId": "76534ba2-4ec3-4a84-ffc2-5e19becd9632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[121, 17],\n",
              " [121, 17, 313],\n",
              " [121, 17, 313, 130],\n",
              " [121, 17, 313, 130, 200],\n",
              " [121, 17, 313, 130, 200, 90],\n",
              " [121, 17, 313, 130, 200, 90, 31],\n",
              " [121, 17, 313, 130, 200, 90, 31, 91],\n",
              " [91, 91],\n",
              " [3, 28],\n",
              " [3, 28, 35],\n",
              " [3, 28, 35, 314],\n",
              " [3, 28, 35, 314, 109],\n",
              " [3, 28, 35, 314, 109, 2],\n",
              " [3, 28, 35, 314, 109, 2, 258],\n",
              " [3, 28, 35, 314, 109, 2, 258, 36],\n",
              " [3, 28, 35, 314, 109, 2, 258, 36, 2],\n",
              " [3, 28, 35, 314, 109, 2, 258, 36, 2, 871],\n",
              " [314, 314],\n",
              " [153, 3],\n",
              " [153, 3, 80],\n",
              " [153, 3, 80, 140],\n",
              " [153, 3, 80, 140, 40],\n",
              " [153, 3, 80, 140, 40, 22],\n",
              " [153, 3, 80, 140, 40, 22, 872],\n",
              " [153, 3, 80, 140, 40, 22, 872, 217]]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_sequence_len = max([len(x) for x in inputSequences])\n",
        "inputSequences = np.array(pad_sequences(inputSequences, maxlen=max_sequence_len, padding=\"pre\"))\n",
        "inputSequences[:15,4:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7UKOCg5z971",
        "outputId": "026e2379-3b58-4bee-bddb-7742d3254d30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0, 121,  17],\n",
              "       [  0,   0,   0,   0,   0,   0, 121,  17, 313],\n",
              "       [  0,   0,   0,   0,   0, 121,  17, 313, 130],\n",
              "       [  0,   0,   0,   0, 121,  17, 313, 130, 200],\n",
              "       [  0,   0,   0, 121,  17, 313, 130, 200,  90],\n",
              "       [  0,   0, 121,  17, 313, 130, 200,  90,  31],\n",
              "       [  0, 121,  17, 313, 130, 200,  90,  31,  91],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  91,  91],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   3,  28],\n",
              "       [  0,   0,   0,   0,   0,   0,   3,  28,  35],\n",
              "       [  0,   0,   0,   0,   0,   3,  28,  35, 314],\n",
              "       [  0,   0,   0,   0,   3,  28,  35, 314, 109],\n",
              "       [  0,   0,   0,   3,  28,  35, 314, 109,   2],\n",
              "       [  0,   0,   3,  28,  35, 314, 109,   2, 258],\n",
              "       [  0,   3,  28,  35, 314, 109,   2, 258,  36]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = inputSequences[:,:-1] # all rows, all expect last column\n",
        "labels = inputSequences[:,-1]  # all rows, last column\n",
        "y = tf.keras.utils.to_categorical(labels, num_classes=total_words)\n",
        "print(\"Input dataset size: \",x.shape) # N# of train sentences by N# of features \n",
        "print(\"Output dataset size: \",y.shape) # N# of train sentences by N# of words"
      ],
      "metadata": {
        "id": "lp7n740U6fZs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc413bbe-5088-47c1-a673-22a7ee784f1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input dataset size:  (8978, 12)\n",
            "Output dataset size:  (8978, 2316)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sentence: \",corpus[0])\n",
        "print(\"Sequence: \",tokenizer.texts_to_sequences([corpus[0]]))\n",
        "print(\"X: \",x[0])\n",
        "print(\"Label: \",labels[0])\n",
        "print(\"Y: \",y[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOJDRcFA8meL",
        "outputId": "0d14b9d0-78ca-4f18-dd73-cf1fd0a06d63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence:  before we proceed any further, hear me speak.\n",
            "Sequence:  [[121, 17, 313, 130, 200, 90, 31, 91]]\n",
            "X:  [  0   0   0   0   0   0   0   0   0   0   0 121]\n",
            "Label:  17\n",
            "Y:  [0. 0. 0. ... 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(150)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "adam = Adam()\n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "history = model.fit(x, y, epochs=60, verbose=1)\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-gL0-pz_OLY",
        "outputId": "df2292ae-1822-4bec-8289-b15d86c37ecf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60\n",
            "281/281 [==============================] - 21s 59ms/step - loss: 6.7390 - accuracy: 0.0394\n",
            "Epoch 2/60\n",
            "281/281 [==============================] - 17s 59ms/step - loss: 6.2473 - accuracy: 0.0451\n",
            "Epoch 3/60\n",
            "281/281 [==============================] - 20s 70ms/step - loss: 6.0626 - accuracy: 0.0535\n",
            "Epoch 4/60\n",
            "281/281 [==============================] - 18s 63ms/step - loss: 5.8806 - accuracy: 0.0573\n",
            "Epoch 5/60\n",
            "281/281 [==============================] - 18s 65ms/step - loss: 5.6489 - accuracy: 0.0718\n",
            "Epoch 6/60\n",
            "281/281 [==============================] - 16s 58ms/step - loss: 5.3705 - accuracy: 0.0781\n",
            "Epoch 7/60\n",
            "281/281 [==============================] - 17s 60ms/step - loss: 5.0627 - accuracy: 0.0936\n",
            "Epoch 8/60\n",
            "281/281 [==============================] - 16s 57ms/step - loss: 4.7226 - accuracy: 0.1119\n",
            "Epoch 9/60\n",
            "281/281 [==============================] - 17s 60ms/step - loss: 4.3706 - accuracy: 0.1348\n",
            "Epoch 10/60\n",
            "281/281 [==============================] - 17s 61ms/step - loss: 4.0214 - accuracy: 0.1709\n",
            "Epoch 11/60\n",
            "281/281 [==============================] - 17s 59ms/step - loss: 3.6746 - accuracy: 0.2160\n",
            "Epoch 12/60\n",
            "281/281 [==============================] - 17s 59ms/step - loss: 3.3441 - accuracy: 0.2836\n",
            "Epoch 13/60\n",
            "281/281 [==============================] - 16s 58ms/step - loss: 3.0312 - accuracy: 0.3435\n",
            "Epoch 14/60\n",
            "281/281 [==============================] - 17s 59ms/step - loss: 2.7551 - accuracy: 0.3953\n",
            "Epoch 15/60\n",
            "281/281 [==============================] - 17s 60ms/step - loss: 2.4900 - accuracy: 0.4538\n",
            "Epoch 16/60\n",
            "281/281 [==============================] - 17s 61ms/step - loss: 2.2634 - accuracy: 0.5008\n",
            "Epoch 17/60\n",
            "281/281 [==============================] - 17s 60ms/step - loss: 2.0675 - accuracy: 0.5482\n",
            "Epoch 18/60\n",
            "281/281 [==============================] - 17s 61ms/step - loss: 1.8906 - accuracy: 0.5826\n",
            "Epoch 19/60\n",
            "281/281 [==============================] - 18s 64ms/step - loss: 1.7267 - accuracy: 0.6171\n",
            "Epoch 20/60\n",
            "281/281 [==============================] - 16s 58ms/step - loss: 1.5833 - accuracy: 0.6550\n",
            "Epoch 21/60\n",
            "281/281 [==============================] - 16s 59ms/step - loss: 1.4493 - accuracy: 0.6851\n",
            "Epoch 22/60\n",
            "281/281 [==============================] - 17s 61ms/step - loss: 1.3303 - accuracy: 0.7156\n",
            "Epoch 23/60\n",
            "281/281 [==============================] - 17s 59ms/step - loss: 1.2237 - accuracy: 0.7397\n",
            "Epoch 24/60\n",
            "281/281 [==============================] - 17s 60ms/step - loss: 1.1325 - accuracy: 0.7558\n",
            "Epoch 25/60\n",
            "281/281 [==============================] - 17s 60ms/step - loss: 1.0475 - accuracy: 0.7790\n",
            "Epoch 26/60\n",
            "281/281 [==============================] - 17s 60ms/step - loss: 0.9822 - accuracy: 0.7895\n",
            "Epoch 27/60\n",
            "281/281 [==============================] - 17s 59ms/step - loss: 0.9260 - accuracy: 0.7975\n",
            "Epoch 28/60\n",
            "281/281 [==============================] - 17s 61ms/step - loss: 0.8634 - accuracy: 0.8113\n",
            "Epoch 29/60\n",
            "281/281 [==============================] - 16s 57ms/step - loss: 0.8132 - accuracy: 0.8208\n",
            "Epoch 30/60\n",
            "281/281 [==============================] - 17s 60ms/step - loss: 0.7643 - accuracy: 0.8270\n",
            "Epoch 31/60\n",
            "281/281 [==============================] - 17s 60ms/step - loss: 0.7394 - accuracy: 0.8305\n",
            "Epoch 32/60\n",
            "281/281 [==============================] - 17s 60ms/step - loss: 0.7082 - accuracy: 0.8350\n",
            "Epoch 33/60\n",
            "281/281 [==============================] - 18s 65ms/step - loss: 0.6702 - accuracy: 0.8391\n",
            "Epoch 34/60\n",
            "281/281 [==============================] - 17s 60ms/step - loss: 0.6524 - accuracy: 0.8413\n",
            "Epoch 35/60\n",
            "281/281 [==============================] - 17s 60ms/step - loss: 0.6388 - accuracy: 0.8426\n",
            "Epoch 36/60\n",
            "281/281 [==============================] - 18s 62ms/step - loss: 0.6172 - accuracy: 0.8442\n",
            "Epoch 37/60\n",
            "281/281 [==============================] - 17s 62ms/step - loss: 0.6046 - accuracy: 0.8440\n",
            "Epoch 38/60\n",
            "281/281 [==============================] - 18s 64ms/step - loss: 0.5890 - accuracy: 0.8476\n",
            "Epoch 39/60\n",
            "281/281 [==============================] - 17s 62ms/step - loss: 0.5705 - accuracy: 0.8497\n",
            "Epoch 40/60\n",
            "281/281 [==============================] - 17s 60ms/step - loss: 0.5603 - accuracy: 0.8502\n",
            "Epoch 41/60\n",
            "281/281 [==============================] - 18s 62ms/step - loss: 0.5584 - accuracy: 0.8509\n",
            "Epoch 42/60\n",
            "281/281 [==============================] - 18s 62ms/step - loss: 0.5529 - accuracy: 0.8489\n",
            "Epoch 43/60\n",
            "281/281 [==============================] - 18s 62ms/step - loss: 0.5385 - accuracy: 0.8499\n",
            "Epoch 44/60\n",
            "281/281 [==============================] - 17s 61ms/step - loss: 0.5297 - accuracy: 0.8503\n",
            "Epoch 45/60\n",
            "281/281 [==============================] - 17s 60ms/step - loss: 0.5211 - accuracy: 0.8503\n",
            "Epoch 46/60\n",
            "281/281 [==============================] - 17s 61ms/step - loss: 0.5193 - accuracy: 0.8506\n",
            "Epoch 47/60\n",
            "281/281 [==============================] - 18s 65ms/step - loss: 0.5220 - accuracy: 0.8499\n",
            "Epoch 48/60\n",
            "281/281 [==============================] - 17s 61ms/step - loss: 0.5183 - accuracy: 0.8486\n",
            "Epoch 49/60\n",
            "281/281 [==============================] - 17s 61ms/step - loss: 0.5137 - accuracy: 0.8507\n",
            "Epoch 50/60\n",
            "281/281 [==============================] - 17s 62ms/step - loss: 0.5122 - accuracy: 0.8495\n",
            "Epoch 51/60\n",
            "281/281 [==============================] - 17s 60ms/step - loss: 0.5014 - accuracy: 0.8497\n",
            "Epoch 52/60\n",
            "281/281 [==============================] - 18s 62ms/step - loss: 0.5039 - accuracy: 0.8495\n",
            "Epoch 53/60\n",
            "281/281 [==============================] - 17s 60ms/step - loss: 0.5024 - accuracy: 0.8504\n",
            "Epoch 54/60\n",
            "281/281 [==============================] - 17s 60ms/step - loss: 0.4962 - accuracy: 0.8525\n",
            "Epoch 55/60\n",
            "281/281 [==============================] - 17s 59ms/step - loss: 0.4916 - accuracy: 0.8513\n",
            "Epoch 56/60\n",
            "281/281 [==============================] - 17s 60ms/step - loss: 0.4887 - accuracy: 0.8499\n",
            "Epoch 57/60\n",
            "281/281 [==============================] - 17s 60ms/step - loss: 0.4856 - accuracy: 0.8523\n",
            "Epoch 58/60\n",
            "281/281 [==============================] - 16s 58ms/step - loss: 0.4891 - accuracy: 0.8491\n",
            "Epoch 59/60\n",
            "281/281 [==============================] - 17s 60ms/step - loss: 0.4854 - accuracy: 0.8507\n",
            "Epoch 60/60\n",
            "281/281 [==============================] - 17s 62ms/step - loss: 0.4824 - accuracy: 0.8513\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 12, 100)           231600    \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 300)              301200    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2316)              697116    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,229,916\n",
            "Trainable params: 1,229,916\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"shall I compare\"\n",
        "text_size = len(seed_text.split(\" \"))\n",
        "line_length = 8\n",
        "next_words = 60\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = np.argmax(model.predict(token_list), axis=-1) \n",
        " \t#perform the sum accross the last axis\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items(): \n",
        "\t\t#the words are the keys and index are the values\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tif text_size<line_length:\n",
        "\t\tseed_text += \" \" + output_word\n",
        "\t\ttext_size=text_size+1\n",
        "\telse:\n",
        "\t\tseed_text += \"\\n\" + output_word\n",
        "\t\ttext_size = 1\n",
        "print(seed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4aIwv0PBLE-",
        "outputId": "bd3c9afb-62dd-49da-ae27-ad21cd848db3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shall I compare had children's voices neither they\n",
            "did together to do it tune suppliants cup\n",
            "controversy thus he shall to their blood wherein\n",
            "he show their love of his bed where\n",
            "on you curse them that say not as\n",
            "a little of a necessary parts being proud\n",
            "parts when he drop will not that have\n",
            "set me his voices host whereof to\n"
          ]
        }
      ]
    }
  ]
}